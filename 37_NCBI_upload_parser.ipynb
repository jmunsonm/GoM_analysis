{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e8b1375-d1dc-48c8-9fd8-356fd347f63d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder already exists\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "sys.path.insert(0, '/home/jmunson-mcgee/')\n",
    "from JMM_functions import *\n",
    "\n",
    "pd.set_option(\"display.max_rows\", 6)\n",
    "\n",
    "outdir='/mnt/scgc/simon/microg2p/Data/GoM_uploads/NCBI_submission_files/split_files'\n",
    "safe_make_dir(outdir)\n",
    "os.chdir(outdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa0d6bc-7b38-4825-8fca-7352593ebdce",
   "metadata": {},
   "source": [
    "## Create folders and txt files for NCBI submission\n",
    "\n",
    "### This notebook needs to have three things\n",
    "1. a biosample attributes txt file that has all of the biosample data for every SAG that is going to be submitted to NCBI \n",
    "1. a genome attributes txt file that has assembly data for every sample \n",
    "    - This file must be in the same order as the biosample_attributes file above to ensure that each \"chunk\" contains the same SAGs\n",
    "1. A single folder with all of the SAG fasta files that will be uploaded\n",
    "\n",
    "### This notebook will create \n",
    "1. biosample_attribute chunk files with 400 SAGs each\n",
    "1. Genome_attribute chunk files with 400 SAGs each\n",
    "1. A folder for each chunk with the same SAGs as the chunk files previously created. \n",
    "\n",
    "\n",
    "Once these files have been created then you can use ascp command line plugin to upload all of the folders to NCBI. The command will look something like this\n",
    "\n",
    "#Folder with all of your fasta files\n",
    "contigfolders=/mnt/scgc/simon/microg2p/Data/GoM_uploads/NCBI_submission_files/split_files/chunk*\n",
    "\n",
    "for folder in ${contigfolders[@]}\n",
    "do\n",
    "\n",
    "ascp -i /mnt/scgc/simon/microg2p/Data/GoM_uploads/aspera.openssh -QT -l100m -k1 -d ${folder} subasp@upload.ncbi.nlm.nih.gov:uploads/jacob.munsonmcgee_msu.montana.edu_bYXjnbY8/\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "027ca8d9-4d2a-4f0f-922b-59780fd27142",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['USA: East Boothbay ME', 'Atlantic Ocean',\n",
       "       'Pacific Ocean: Juan De Fuca ridge'], dtype=object)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# replace some of the data that was initially missing. It can be omitted from other uploads\n",
    "\n",
    "sample_path = \"/mnt/scgc/simon/microg2p/Data/GoM_uploads/NCBI_submission_files/Biosample_attributes.txt\"\n",
    "sample=pd.read_csv(sample_path, sep='\\t')    \n",
    "sample[['*geo_loc_name', '*isolation_source']]=sample[['*geo_loc_name', '*isolation_source']].fillna(value='Northern Atlantic')\n",
    "sample['*lat_lon']=sample['*lat_lon'].fillna('43.001_-11.333')\n",
    "sample['*geo_loc_name']=sample['*geo_loc_name'].replace(['Northern Atlantic Ocean'], 'Atlantic Ocean')\n",
    "sample['*geo_loc_name']=sample['*geo_loc_name'].replace(['Equitorial atlantic'], 'Atlantic Ocean')\n",
    "sample['*geo_loc_name']=sample['*geo_loc_name'].replace(['Southern Atlantic'], 'Atlantic Ocean')\n",
    "sample['*geo_loc_name']=sample['*geo_loc_name'].replace(['Juan De Fuca ridge'], 'Pacific Ocean: Juan De Fuca ridge')\n",
    "sample.to_csv(sample_path, sep='\\t')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8701e650-a27f-4725-8a69-26db37b49f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,chunk in enumerate(pd.read_csv(sample_path, chunksize=400, sep='\\t')):\n",
    "    chunk.to_csv('/mnt/scgc/simon/microg2p/Data/GoM_uploads/NCBI_submission_files/split_files/biosample_attributes_chunk{}.txt'.format(i), index=False, sep='\\t')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27ab7578-078f-4400-bb4c-844aba3b4cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "genome_path = \"/mnt/scgc/simon/microg2p/Data/GoM_uploads/NCBI_submission_files/SAG_genome_attributes.txt\"\n",
    "for i,chunk in enumerate(pd.read_csv(genome_path, chunksize=400, sep='\\t')):\n",
    "    chunk.to_csv('/mnt/scgc/simon/microg2p/Data/GoM_uploads/NCBI_submission_files/split_files/SAG_genome_attributes_chunk{}.txt'.format(i), index=False, sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "010e430b-4e51-459d-bab9-8c26c22c1cd4",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "There is probably a good argument to be made that I should do the chunk once on the biosample_attributes file and then use those chunk files to extract the SAGs that are in a chunk file from the genome_attributes file and create the first genome_attributes chunk file in a similar fashion to how I created the fasta chunk folders.\n",
    "Oh well for now I am not going to modify the code but I will think about it for the future.\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e479da3a-ec3b-4c4b-b35c-bf49823bdbfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder created\n",
      "400 files were copied to chunk8_SAG_contigs\n",
      "Folder created\n",
      "400 files were copied to chunk9_SAG_contigs\n",
      "Folder created\n",
      "400 files were copied to chunk0_SAG_contigs\n",
      "Folder created\n",
      "400 files were copied to chunk1_SAG_contigs\n",
      "Folder created\n",
      "400 files were copied to chunk2_SAG_contigs\n",
      "Folder created\n",
      "400 files were copied to chunk3_SAG_contigs\n",
      "Folder created\n",
      "400 files were copied to chunk4_SAG_contigs\n",
      "Folder created\n",
      "400 files were copied to chunk5_SAG_contigs\n",
      "Folder created\n",
      "400 files were copied to chunk6_SAG_contigs\n",
      "Folder created\n",
      "400 files were copied to chunk7_SAG_contigs\n",
      "Folder created\n",
      "400 files were copied to chunk10_SAG_contigs\n",
      "Folder created\n",
      "400 files were copied to chunk11_SAG_contigs\n",
      "Folder created\n",
      "400 files were copied to chunk12_SAG_contigs\n",
      "Folder created\n",
      "400 files were copied to chunk13_SAG_contigs\n",
      "Folder created\n",
      "400 files were copied to chunk14_SAG_contigs\n",
      "Folder created\n",
      "400 files were copied to chunk15_SAG_contigs\n",
      "Folder created\n",
      "400 files were copied to chunk16_SAG_contigs\n",
      "Folder created\n",
      "400 files were copied to chunk17_SAG_contigs\n",
      "Folder created\n",
      "318 files were copied to chunk18_SAG_contigs\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import shutil\n",
    "\n",
    "# gather all of the SAG files\n",
    "contigfiles=glob.glob('/mnt/scgc/simon/microg2p/Data/GoM_uploads/SAG_contigs/*.fasta')\n",
    "len(contigfiles)\n",
    "\n",
    "# gather all of the tables that were previously produced\n",
    "tablefiles=glob.glob('/mnt/scgc/simon/microg2p/Data/GoM_uploads/NCBI_submission_files/split_files/SAG_genome_attributes_chunk*.txt')\n",
    "len(tablefiles)\n",
    "\n",
    "# remove all folders with SAG contigs yes this is brute force and makes it slower \n",
    "# but it also ensures that only the files within each table are in the folders\n",
    "! rm -r *contigs\n",
    "\n",
    "# for each table (chunk) do the following\n",
    "#     1. extract the chunk # to keep it consistent with the table in case it is not read from 0-18\n",
    "#     2. empty anything that is already in that folder with that chunk# and remake it\n",
    "#     3. Then read the table and extract the SAGs that are part of it \n",
    "#     4. for every SAG check if the SAG name is in the fasta name\n",
    "#         if it is copy it to the new folder and append it to the list\n",
    "        \n",
    "for file in tablefiles:\n",
    "    path,chunk=file.rsplit('_', 1)\n",
    "    chunk=chunk[:-4]\n",
    "    \n",
    "    folder=chunk+\"_SAG_contigs\"\n",
    "    safe_make_dir(folder)\n",
    "    \n",
    "    df=pd.read_csv(file, sep='\\t')\n",
    "    names=df['filename'].to_list()\n",
    "    cpfiles=[]\n",
    "    for SAG in names:\n",
    "        for i in contigfiles:\n",
    "            if SAG in i:\n",
    "                if i not in folder:\n",
    "                    shutil.copy(i, folder)\n",
    "                    cpfiles.append(i)\n",
    "    print(len(glob.glob(folder+'/*')), \"files were copied to\", folder)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ae27611b-2792-43fd-a00b-5e7380456ae8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "318"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cfa4417-6054-4d7a-8478-19c18d04793a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Jacob",
   "language": "python",
   "name": "jacob"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
